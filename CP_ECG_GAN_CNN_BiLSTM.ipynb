{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CP_ECG_GAN_CNN_BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LojP7F2a5zCy"
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.activations import relu\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import scipy.stats as sc\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kle6y1anHPca"
      },
      "source": [
        "optimiser_d = Adam(0.001)\n",
        "optimiser_g = Adam(0.000005)\n",
        "ecg_shape = (400,1)\n",
        "noise_length = 400\n",
        "\n",
        "\n",
        "def create_discriminator():\n",
        "  discriminator = Sequential()\n",
        "  Input_shape = (400,1)\n",
        "\n",
        "  discriminator.add(Reshape((400,1),input_shape = (ecg_shape)))\n",
        "\n",
        "  discriminator.add(Conv1D(filters = 64, kernel_size = 3, strides = 1))\n",
        "  discriminator.add(MaxPooling1D(pool_size = 3, strides = 1))\n",
        "  discriminator.add(LeakyReLU())\n",
        "\n",
        "\n",
        "  discriminator.add(Conv1D(filters = 128, kernel_size = 3, strides = 1))\n",
        "  discriminator.add(MaxPooling1D(pool_size = 3, strides = 1))\n",
        "  discriminator.add(LeakyReLU())\n",
        "\n",
        "\n",
        "  discriminator.add(Conv1D(filters = 256, kernel_size = 3, strides = 1))\n",
        "  discriminator.add(MaxPooling1D(pool_size = 3, strides = 2))\n",
        "  discriminator.add(LeakyReLU())\n",
        "\n",
        "\n",
        "  discriminator.add(Conv1D(filters = 512, kernel_size = 3, strides = 1))\n",
        "  discriminator.add(MaxPooling1D(pool_size = 3, strides = 2))\n",
        "  discriminator.add(LeakyReLU())\n",
        "\n",
        "\n",
        "  discriminator.add(Flatten())\n",
        "   \n",
        "  discriminator.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "  discriminator.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optimiser_d)\n",
        "  discriminator.summary()\n",
        "\n",
        "  return discriminator\n",
        "\n",
        "def create_generator():\n",
        "\n",
        "  generator = Sequential()\n",
        "\n",
        "  Input_shape = (400,1)\n",
        "  generator.add(Bidirectional(LSTM(64, return_sequences=True), input_shape = Input_shape))\n",
        "  generator.add(Dropout(0.1))\n",
        "  generator.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "  generator.add(Dropout(0.1))\n",
        "\n",
        "  generator.add(Flatten())\n",
        "  generator.add(Dense(ecg_shape[0], activation='tanh'))\n",
        "\n",
        "  generator.compile(loss='binary_crossentropy', optimizer=optimiser_g)\n",
        "  generator.summary()\n",
        "\n",
        "  return generator\n",
        "\n",
        "\n",
        "def create_GAN(discriminator, generator):\n",
        "  discriminator.trainable = False\n",
        "\n",
        "  GAN = Sequential()\n",
        "  GAN.add(Input(shape=(400,1)))\n",
        "  GAN.add(generator)\n",
        "  GAN.add(discriminator)\n",
        "\n",
        "  GAN.compile(loss='binary_crossentropy', optimizer=optimiser_g)\n",
        "  GAN.summary()\n",
        "  return GAN\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VujR1b1M1N_"
      },
      "source": [
        "def show_results(generator,norm_value):\n",
        "  noise = tf.random.normal([400,1])\n",
        "  print(noise.shape)\n",
        "  noise1 = tf.expand_dims(noise, axis=0)\n",
        "  print(noise1.shape)\n",
        "  ecgs = generator.predict(noise1)\n",
        "  denorm_ecgs = ecgs * norm_value\n",
        "\n",
        "  #plt.plot(noise)\n",
        "  plt.figure()\n",
        "  plt.ylabel('Amplitude(mV)')\n",
        "  plt.xlabel('Timesteps')\n",
        "  plt.xlim((0,400))\n",
        "  plt.plot(denorm_ecgs[0])\n",
        "  plt.grid()\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdVBleh_lKZ5"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def compute_kernel(x, y):\n",
        "    x_size = tf.shape(x)[0]\n",
        "    y_size = tf.shape(y)[0]\n",
        "    dim = tf.shape(x)[1]\n",
        "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
        "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
        "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float64))\n",
        "\n",
        "def compute_mmd(x, y, sigma_sqr=1.0):\n",
        "    x_kernel = compute_kernel(x, x)\n",
        "    y_kernel = compute_kernel(y, y)\n",
        "    xy_kernel = compute_kernel(x, y)\n",
        "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_eyysd2lT0I"
      },
      "source": [
        "\n",
        "def mmd_loss():\n",
        "\n",
        "  real_data_set =  np.load('')\n",
        "  idx = np.random.randint(0, 1000, 800)\n",
        "  real_data = real_data_set[:,idx]\n",
        "\n",
        "  #print(real_data.shape)\n",
        "\n",
        "  fake_data = np.zeros((400,800))\n",
        "  data = np.zeros(400)\n",
        "  for i in range(800):\n",
        "    noise = tf.random.normal([400,1])\n",
        "    #print(noise.shape)\n",
        "    noise1 = tf.expand_dims(noise, axis=0)\n",
        "    #print(noise1.shape)\n",
        "    ecgs = generator.predict(noise1)\n",
        "    denorm_ecgs = ecgs * norm_value\n",
        "\n",
        "    for j in range(400):\n",
        "      data[j] = denorm_ecgs[0,j]\n",
        "    fake_data[:,i] = data\n",
        "\n",
        "\n",
        "  real_data = np.transpose(real_data)\n",
        "  fake_data = np.transpose(fake_data)\n",
        "\n",
        "  mmd_loss = compute_mmd(real_data,fake_data)\n",
        "  print(mmd_loss)\n",
        "\n",
        "  return mmd_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjCztqJ49eqD"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 5000\n",
        "discriminator = create_discriminator()\n",
        "generator = create_generator()\n",
        "GAN = create_GAN(discriminator, generator)\n",
        "\n",
        "data = np.load('')\n",
        "maxim = np.zeros(len(data))\n",
        "minim = np.zeros(len(data))\n",
        "#scale = np.zeros(2*len(data))\n",
        "\n",
        "for I in range(len(data)):\n",
        "  maxim[I] = max(data[I])\n",
        "  minim[I] = -min(data[I])\n",
        "#print(max(maxim))\n",
        "#print(max(minim))\n",
        "scale = np.concatenate([maxim,minim])\n",
        "norm_value = max(scale)\n",
        "data = data / norm_value\n",
        "noise_array =  np.array(tf.random.normal([1000, 400, 1]))\n",
        "#rint(noise_array.shape)\n",
        "\n",
        "d_losses, d_accuracy, g_losses = [], [], []\n",
        "\n",
        "mmd_losses = np.zeros(50)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  idx = np.random.randint(0, high = 1000 , size= batch_size)\n",
        "  real_ecgs = data[:,idx]\n",
        "  real_ecgs = np.transpose(real_ecgs)\n",
        "  real_ecgs = tf.expand_dims(real_ecgs, axis=2)\n",
        "\n",
        "  idx = np.random.randint(0,high= 1000 ,size=batch_size)\n",
        "  noise = noise_array[idx,:,:]\n",
        "\n",
        "  fake_ecgs = generator.predict(noise,batch_size=batch_size)\n",
        "  fake_ecgs = tf.expand_dims(fake_ecgs, axis=2)\n",
        "\n",
        "  X_real = real_ecgs\n",
        "  X_fake = fake_ecgs\n",
        "\n",
        "  y_real = np.ones(batch_size)\n",
        "  y_fake = np.zeros(batch_size)\n",
        "\n",
        "  discriminator.trainable = True\n",
        "  d_loss_real = discriminator.train_on_batch(X_real ,y_real)\n",
        "  d_loss_fake = discriminator.train_on_batch(X_fake ,y_fake)\n",
        "  d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "  discriminator.trainable = False\n",
        "\n",
        "  y = np.ones(batch_size)\n",
        "  g_loss = GAN.train_on_batch(noise, y)\n",
        "\n",
        "  d_losses.append(d_loss[0])\n",
        "  d_accuracy.append(d_loss[1])\n",
        "  g_losses.append(g_loss)\n",
        "\n",
        "  if epoch%20 == 0:\n",
        "    show_results(generator,norm_value)  \n",
        "    show_results(generator,norm_value)\n",
        "    show_results(generator,norm_value)\n",
        "    show_results(generator,norm_value)\n",
        "    show_results(generator,norm_value)\n",
        "    show_results(generator,norm_value)\n",
        "    show_results(generator,norm_value)\n",
        "    mmd_loss()\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel('Loss Value')\n",
        "plt.xlabel('Epochs')\n",
        "plt.xlim((0,5000))\n",
        "plt.plot(d_losses, 'r', label='disc_loss')\n",
        "plt.plot(g_losses, 'b', label='gen_loss')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(d_accuracy)\n",
        "plt.show()\n",
        "\n",
        "plt.plot(mmd_losses)\n",
        "plt.show()\n",
        "\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkXajTw3lqyD"
      },
      "source": [
        "mmd_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PLgdBJghZdb"
      },
      "source": [
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}