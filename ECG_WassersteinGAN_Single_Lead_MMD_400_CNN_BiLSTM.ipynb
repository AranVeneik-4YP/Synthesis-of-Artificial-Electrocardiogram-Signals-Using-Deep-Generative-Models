{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECG_WassersteinGAN_Single Lead_MMD_400_CNN_BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LojP7F2a5zCy"
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.activations import relu\n",
        "from keras import backend\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import scipy.stats as sc\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kle6y1anHPca"
      },
      "source": [
        "optimiser = RMSprop(0.00005)\n",
        "ecg_shape = (400,1)\n",
        "n_critic = 7\n",
        "clip_value = 0.02\n",
        "\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_true * y_pred)\n",
        " \n",
        "\n",
        "def create_critic():\n",
        "  critic = Sequential()\n",
        "  Input_shape = (400,1)\n",
        "  critic.add(Reshape((400,1),input_shape = (ecg_shape)))\n",
        "  # Conv 1\n",
        "  critic.add(Conv1D(64, kernel_size=5, strides=1,input_shape=(ecg_shape)))\n",
        "  critic.add(MaxPooling1D(pool_size = 5, strides = 1))\n",
        "  critic.add(LeakyReLU(0.2))\n",
        "\n",
        "  # Conv 2\n",
        "  critic.add(Conv1D(128, kernel_size=5, strides=1))\n",
        "  critic.add(MaxPooling1D(pool_size = 5, strides = 1))\n",
        "  #critic.add(BatchNormalization(momentum=0.9))\n",
        "  critic.add(LeakyReLU(0.2))\n",
        "\n",
        "  # Conv 3\n",
        "  critic.add(Conv1D(256, kernel_size=5, strides=1))\n",
        "  critic.add(MaxPooling1D(pool_size = 5, strides = 2))\n",
        "  #critic.add(BatchNormalization(momentum=0.9))\n",
        "  critic.add(LeakyReLU(0.2))\n",
        "\n",
        "  # Conv 4\n",
        "  critic.add(Conv1D(512, kernel_size=5, strides=1))\n",
        "  critic.add(MaxPooling1D(pool_size = 5, strides = 2))\n",
        "  #critic.add(BatchNormalization(momentum=0.9))\n",
        "  critic.add(LeakyReLU(0.2))\n",
        "\n",
        "  critic.add(Flatten())\n",
        "  \n",
        "  critic.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "  critic.compile(loss=wasserstein_loss, metrics=['accuracy'], optimizer=optimiser)\n",
        "  critic.summary()\n",
        "\n",
        "  return critic\n",
        "\n",
        "def create_generator():\n",
        "\n",
        "  generator = Sequential()\n",
        "\n",
        "  Input_shape = (400,1)\n",
        "\n",
        "  generator.add(Bidirectional(LSTM(128, return_sequences=True), input_shape = Input_shape))\n",
        "  generator.add(Dropout(0.1))\n",
        "  generator.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "  generator.add(Dropout(0.1))\n",
        "\n",
        "   \n",
        "  generator.add(Flatten())\n",
        "  generator.add(Dense(ecg_shape[0], activation='tanh'))\n",
        "\n",
        "  #generator.compile(loss=wasserstein_loss, optimizer=optimiser)\n",
        "  generator.summary()\n",
        "\n",
        "  return generator\n",
        "\n",
        "\n",
        "def create_GAN(critic, generator):\n",
        "  critic.trainable = False\n",
        "\n",
        "  GAN = Sequential()\n",
        "  GAN.add(Input(shape=(400,1)))\n",
        "  GAN.add(generator)\n",
        "  GAN.add(critic)\n",
        "\n",
        "  GAN.compile(loss=wasserstein_loss, optimizer=optimiser)\n",
        "  GAN.summary()\n",
        "  return GAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VujR1b1M1N_"
      },
      "source": [
        "def show_results(generator,norm_value):\n",
        "  noise = tf.random.normal([400,1])\n",
        "  print(noise.shape)\n",
        "  noise1 = tf.expand_dims(noise, axis=0)\n",
        "  print(noise1.shape)\n",
        "  ecgs = generator.predict(noise1)\n",
        "  denorm_ecgs = ecgs * norm_value\n",
        "\n",
        "  #plt.plot(noise)\n",
        "  plt.plot(denorm_ecgs[0])\n",
        "  plt.grid()\n",
        "  plt.xlabel('Time step')\n",
        "  plt.ylabel('Amplitude(mV)')\n",
        "  plt.xlim((0,400))\n",
        "  plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdVBleh_lKZ5"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def compute_kernel(x, y):\n",
        "    x_size = tf.shape(x)[0]\n",
        "    y_size = tf.shape(y)[0]\n",
        "    dim = tf.shape(x)[1]\n",
        "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
        "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
        "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float64))\n",
        "\n",
        "def compute_mmd(x, y, sigma_sqr=1.0):\n",
        "    x_kernel = compute_kernel(x, x)\n",
        "    y_kernel = compute_kernel(y, y)\n",
        "    xy_kernel = compute_kernel(x, y)\n",
        "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_eyysd2lT0I"
      },
      "source": [
        "\n",
        "def mmd_loss():\n",
        "\n",
        "  data_1 = np.load('/content/gdrive/My Drive/4YP/Data/ECG_Real_2.npy')\n",
        "  data_2 = np.load('/content/gdrive/My Drive/4YP/Data/ECG_Real_2_Test.npy')\n",
        "  data_3 = np.load('/content/gdrive/My Drive/4YP/Data/ECG_Real_2_Extra.npy')\n",
        "  real_data_set = np.concatenate([data_1,data_2,data_3],axis =1)\n",
        "  real_data = real_data_set[:,2000:2800]\n",
        "\n",
        "  #print(real_data.shape)\n",
        "\n",
        "  fake_data = np.zeros((400,800))\n",
        "  data = np.zeros(400)\n",
        "  for i in range(800):\n",
        "    noise = tf.random.normal([400,1])\n",
        "    #print(noise.shape)\n",
        "    noise1 = tf.expand_dims(noise, axis=0)\n",
        "    #print(noise1.shape)\n",
        "    ecgs = generator.predict(noise1)\n",
        "    denorm_ecgs = ecgs * norm_value\n",
        "\n",
        "    for j in range(400):\n",
        "      data[j] = denorm_ecgs[0,j]\n",
        "    fake_data[:,i] = data\n",
        "\n",
        "\n",
        "  real_data = np.transpose(real_data)\n",
        "  #print(real_data.shape)\n",
        "  fake_data = np.transpose(fake_data)\n",
        "  #print(fake_data.shape)\n",
        "\n",
        "  mmd_loss = compute_mmd(real_data,fake_data)\n",
        "  print(mmd_loss)\n",
        "\n",
        "  return mmd_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjCztqJ49eqD"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 5000\n",
        "critic = create_critic()\n",
        "generator = create_generator()\n",
        "GAN = create_GAN(critic, generator)\n",
        "\n",
        "data_1 = np.load('/content/gdrive/My Drive/4YP/Data/ECG_Real_2.npy')\n",
        "data_2 = np.load('/content/gdrive/My Drive/4YP/Data/ECG_Real_2_Test.npy')\n",
        "data_3 = np.load('/content/gdrive/My Drive/4YP/Data/ECG_Real_2_Extra.npy')\n",
        "data = np.concatenate([data_1,data_2,data_3],axis =1)\n",
        "data1 = data[:,:2000]\n",
        "data2 = data[:,3000:]\n",
        "data = np.concatenate([data1,data2],axis =1)\n",
        "maxim = np.zeros(len(data))\n",
        "minim = np.zeros(len(data))\n",
        "#scale = np.zeros(2*len(data))\n",
        "\n",
        "for I in range(len(data)):\n",
        "  maxim[I] = max(data[I])\n",
        "  minim[I] = -min(data[I])\n",
        "#print(max(maxim))\n",
        "#print(max(minim))\n",
        "scale = np.concatenate([maxim,minim])\n",
        "norm_value = max(scale)\n",
        "data = data / norm_value\n",
        "noise_array =  np.array(tf.random.normal([1000, 400, 1]))\n",
        "#rint(noise_array.shape)\n",
        "\n",
        "d_losses, d_accuracy, g_losses = [], [], []\n",
        "\n",
        "mmd_losses = np.zeros(50)\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "\n",
        "  for _ in range(n_critic):\n",
        "    \n",
        "    idx = np.random.randint(0, high = 954 , size= batch_size)\n",
        "    real_ecgs = data[:,idx]\n",
        "    real_ecgs = np.transpose(real_ecgs)\n",
        "    real_ecgs = tf.expand_dims(real_ecgs, axis=2)\n",
        "\n",
        "    idx = np.random.randint(0,high= 1000 ,size=batch_size)\n",
        "    noise = noise_array[idx,:,:]\n",
        "\n",
        "    fake_ecgs = generator.predict(noise,batch_size=batch_size)\n",
        "    fake_ecgs = tf.expand_dims(fake_ecgs, axis=2)\n",
        "\n",
        "    X_real = real_ecgs\n",
        "    X_fake = fake_ecgs\n",
        "\n",
        "    y_real = -np.ones(batch_size) \n",
        "    y_fake = np.ones(batch_size) \n",
        "\n",
        "    critic.trainable = True\n",
        "    c_loss_real = critic.train_on_batch(X_real ,y_real)\n",
        "    c_loss_fake = critic.train_on_batch(X_fake ,y_fake)\n",
        "    critic.trainable = False\n",
        "\n",
        "    for l in critic.layers:\n",
        "        weights = l.get_weights()\n",
        "        weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
        "        l.set_weights(weights)\n",
        "\n",
        "  y = - np.ones(batch_size)\n",
        "  g_loss = GAN.train_on_batch(noise, y)\n",
        "\n",
        "\n",
        "  #c_losses.append(c_loss[0])\n",
        "  #c_accuracy.append(c_loss[1])\n",
        "  g_losses.append(g_loss)\n",
        "\n",
        "\n",
        "  if epoch%100 == 0:\n",
        "    show_results(generator,norm_value)  \n",
        "    show_results(generator,norm_value)\n",
        "    show_results(generator,norm_value)\n",
        "    show_results(generator,norm_value)\n",
        "    mmd_loss()\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel('Loss Value')\n",
        "plt.xlabel('Epochs')\n",
        "plt.xlim((0,5000))\n",
        "plt.plot(d_losses, 'r', label='disc_loss')\n",
        "plt.plot(g_losses, 'b', label='gen_loss')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(d_accuracy)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PLgdBJghZdb"
      },
      "source": [
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)\n",
        "show_results(generator,norm_value)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_v-nbPCOP34g"
      },
      "source": [
        "mmd_loss()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}