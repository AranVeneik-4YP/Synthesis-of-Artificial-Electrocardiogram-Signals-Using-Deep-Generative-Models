{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECG_WGAN_3_Lead_Hybrid_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LojP7F2a5zCy"
      },
      "source": [
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.activations import relu\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import scipy.stats as sc\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from keras import backend\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGr53_DdxtT6"
      },
      "source": [
        "\n",
        "data_1 = np.load('')\n",
        "data_2 = np.load('')\n",
        "data_3 = np.load('')\n",
        "\n",
        "\n",
        "\n",
        "maxim = np.zeros(1000)\n",
        "minim = np.zeros(1000)\n",
        "for I in range(954):\n",
        "  maxim[I] = max(data_1[:,I])\n",
        "  minim[I] = -min(data_1[:,I])\n",
        "scale = np.concatenate([maxim,minim])\n",
        "norm_value_1 = max(scale)\n",
        "data_1 = data_1 / norm_value_1\n",
        "\n",
        "maxim = np.zeros(1000)\n",
        "minim = np.zeros(1000)\n",
        "for I in range(954):\n",
        "  maxim[I] = max(data_2[:,I])\n",
        "  minim[I] = -min(data_2[:,I])\n",
        "scale = np.concatenate([maxim,minim])\n",
        "norm_value_2 = max(scale)\n",
        "data_2 = data_2 / norm_value_2\n",
        "\n",
        "maxim = np.zeros(1000)\n",
        "minim = np.zeros(1000)\n",
        "for I in range(954):\n",
        "  maxim[I] = max(data_3[:,I])\n",
        "  minim[I] = -min(data_3[:,I])\n",
        "scale = np.concatenate([maxim,minim])\n",
        "norm_value_3 = max(scale)\n",
        "data_3 = data_3 / norm_value_3\n",
        "\n",
        "\n",
        "data = np.zeros((954,400,3))\n",
        "\n",
        "for i in range(954):\n",
        "  data[i,:,0] = data_1[:,i] \n",
        "  data[i,:,1] = data_2[:,i] \n",
        "  data[i,:,2] = data_3[:,i] \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kle6y1anHPca"
      },
      "source": [
        "optimiser = RMSprop(0.00005)\n",
        "ecg_shape = (400,3)\n",
        "n_critic = 4\n",
        "clip_value = 0.01\n",
        "\n",
        "\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_true * y_pred)\n",
        " \n",
        "\n",
        "def create_critic():\n",
        "  critic = Sequential()\n",
        "  Input_shape = (400,3)\n",
        "  critic.add(Reshape((400,3),input_shape = (ecg_shape)))\n",
        "  # Conv 1\n",
        "  critic.add(Conv1D(64, kernel_size=5, strides=1,input_shape=(ecg_shape)))\n",
        "  critic.add(MaxPooling1D(pool_size =5, strides = 1))\n",
        "  critic.add(LeakyReLU(0.2))\n",
        "\n",
        "  # Conv 2\n",
        "  critic.add(Conv1D(128, kernel_size=5, strides=1))\n",
        "  critic.add(MaxPooling1D(pool_size = 5, strides = 1))\n",
        "  critic.add(LeakyReLU(0.2))\n",
        "\n",
        "  # Conv 3\n",
        "  critic.add(Conv1D(256, kernel_size=5, strides=1))\n",
        "  critic.add(MaxPooling1D(pool_size = 5, strides = 2))\n",
        "  critic.add(LeakyReLU(0.2))\n",
        "\n",
        "  # Conv 4\n",
        "  critic.add(Conv1D(512, kernel_size=5, strides=1))\n",
        "  critic.add(MaxPooling1D(pool_size = 5, strides = 2))\n",
        "  critic.add(LeakyReLU(0.2))\n",
        "\n",
        "\n",
        "  critic.add(Flatten())\n",
        "  \n",
        "  critic.add(Dense(units=3, activation='linear'))\n",
        "\n",
        "  critic.compile(loss=wasserstein_loss, metrics=['accuracy'], optimizer=optimiser)\n",
        "  critic.summary()\n",
        "\n",
        "  return critic\n",
        "\n",
        "def create_generator():\n",
        "\n",
        "  Input_shape = (400,1)\n",
        "  noise = Input(shape=Input_shape)\n",
        "  x = Reshape((400,1),input_shape = (400,1))\n",
        "  x = Bidirectional(LSTM(128, return_sequences=True))(noise)\n",
        "\n",
        "  x1 = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "  x1 = Dropout(0.1)(x1)\n",
        "  x1 = Flatten()(x1)\n",
        "  # x1 = Dense(ecg_shape[0], activation='tanh')(x1)\n",
        "  x1 = tf.expand_dims(x1,axis=2)\n",
        "\n",
        "\n",
        "  x2 = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "  x2 = Dropout(0.1)(x2)\n",
        "  x2 = Flatten()(x2)\n",
        "  # x2 = Dense(ecg_shape[0], activation='tanh')(x2)\n",
        "  x2 = tf.expand_dims(x2,axis=2)\n",
        "\n",
        "  x3 = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "  x3 = Dropout(0.1)(x3)\n",
        "  x3 = Flatten()(x3)\n",
        "  # x3 = Dense(ecg_shape[0], activation='tanh')(x3)\n",
        "  x3 = tf.expand_dims(x3,axis=2)\n",
        "\n",
        "  \n",
        "  x_out = Concatenate(axis=2)([x1,x2,x3])\n",
        "  x_out = Permute((2,1))(x_out)\n",
        "  x_out = Dense(400, activation = 'tanh')(x_out)\n",
        "  x_out = Permute((2,1))(x_out) \n",
        "  \n",
        "  generator = Model(noise,x_out)\n",
        "  generator.summary()\n",
        "\n",
        "  return generator\n",
        "\n",
        "\n",
        "def create_GAN(critic, generator):\n",
        "  critic.trainable = False\n",
        "\n",
        "  GAN = Sequential()\n",
        "  GAN.add(Input(shape=(400,1)))\n",
        "  GAN.add(generator)\n",
        "  GAN.add(critic)\n",
        "\n",
        "  GAN.compile(loss=wasserstein_loss, optimizer=optimiser)\n",
        "  GAN.summary()\n",
        "  return GAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w0mDjXpr-bV"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def compute_kernel(x, y):\n",
        "    x_size = tf.shape(x)[0]\n",
        "    y_size = tf.shape(y)[0]\n",
        "    dim = tf.shape(x)[1]\n",
        "    tiled_x = tf.tile(tf.reshape(x, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
        "    tiled_y = tf.tile(tf.reshape(y, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
        "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float64))\n",
        "\n",
        "def compute_mmd(x, y, sigma_sqr=1.0):\n",
        "    x_kernel = compute_kernel(x, x)\n",
        "    y_kernel = compute_kernel(y, y)\n",
        "    xy_kernel = compute_kernel(x, y)\n",
        "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp3bRZOWr9M8"
      },
      "source": [
        "\n",
        "def mmd_loss_1():\n",
        "\n",
        "  real_data_set =  np.load('')\n",
        "  idx = np.random.randint(0, 954, 800)\n",
        "  real_data = real_data_set[:,idx]\n",
        "\n",
        "  #print(real_data.shape)\n",
        "\n",
        "  fake_data = np.zeros((400,800))\n",
        "  data = np.zeros(400)\n",
        "  for i in range(800):\n",
        "    noise = tf.random.normal([400,1])\n",
        "    #print(noise.shape)\n",
        "    noise1 = tf.expand_dims(noise, axis=0)\n",
        "    #print(noise1.shape)\n",
        "    ecgs = generator.predict(noise1)\n",
        "    denorm_ecgs_1 = ecgs[0,:,0] * norm_value_1\n",
        "    #print(denorm_ecgs_1.shape)\n",
        "\n",
        "    for j in range(400):\n",
        "      data[j] = denorm_ecgs_1[j]\n",
        "    fake_data[:,i] = data\n",
        "\n",
        "  real_data = np.transpose(real_data)\n",
        "  fake_data = np.transpose(fake_data)\n",
        "\n",
        "  mmd_loss_1 = compute_mmd(real_data,fake_data)\n",
        "  print(mmd_loss_1)\n",
        "\n",
        "  return mmd_loss_1\n",
        "\n",
        "\n",
        "def mmd_loss_2():\n",
        "\n",
        "  real_data_set =  np.load('')\n",
        "  idx = np.random.randint(0, 954, 800)\n",
        "  real_data = real_data_set[:,idx]\n",
        "\n",
        "  #print(real_data.shape)\n",
        "\n",
        "  fake_data = np.zeros((400,800))\n",
        "  data = np.zeros(400)\n",
        "  for i in range(800):\n",
        "    noise = tf.random.normal([400,1])\n",
        "    #print(noise.shape)\n",
        "    noise1 = tf.expand_dims(noise, axis=0)\n",
        "    #print(noise1.shape)\n",
        "    ecgs = generator.predict(noise1)\n",
        "    denorm_ecgs_2 = ecgs[0,:,1] * norm_value_2\n",
        "\n",
        "    for j in range(400):\n",
        "      data[j] = denorm_ecgs_2[j]\n",
        "    fake_data[:,i] = data\n",
        "\n",
        "\n",
        "  real_data = np.transpose(real_data)\n",
        "  fake_data = np.transpose(fake_data)\n",
        "\n",
        "  mmd_loss_2 = compute_mmd(real_data,fake_data)\n",
        "  print(mmd_loss_2)\n",
        "\n",
        "  return mmd_loss_2\n",
        "\n",
        "\n",
        "def mmd_loss_3():\n",
        "\n",
        "  real_data_set =  np.load('')\n",
        "  idx = np.random.randint(0, 954, 800)\n",
        "  real_data = real_data_set[:,idx]\n",
        "\n",
        "  #print(real_data.shape)\n",
        "\n",
        "  fake_data = np.zeros((400,800))\n",
        "  data = np.zeros(400)\n",
        "  for i in range(800):\n",
        "    noise = tf.random.normal([400,1])\n",
        "    #print(noise.shape)\n",
        "    noise1 = tf.expand_dims(noise, axis=0)\n",
        "    #print(noise1.shape)\n",
        "    ecgs = generator.predict(noise1)\n",
        "    denorm_ecgs_3 = ecgs[0,:,2] * norm_value_3\n",
        "\n",
        "    for j in range(400):\n",
        "      data[j] = denorm_ecgs_3[j]\n",
        "    fake_data[:,i] = data\n",
        "\n",
        "\n",
        "  real_data = np.transpose(real_data)\n",
        "  fake_data = np.transpose(fake_data)\n",
        "\n",
        "  mmd_loss_3 = compute_mmd(real_data,fake_data)\n",
        "  print(mmd_loss_3)\n",
        "\n",
        "  return mmd_loss_3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VujR1b1M1N_"
      },
      "source": [
        "def show_results(generator,norm_value_1,norm_value_2,norm_value_3):\n",
        "  noise = tf.random.normal([400,1])\n",
        "  noise1 = tf.expand_dims(noise, axis=0)\n",
        "  ecgs = generator.predict(noise1)\n",
        "\n",
        "  denorm_ecgs_1 = ecgs[0,:,0] * norm_value_1\n",
        "  denorm_ecgs_2 = ecgs[0,:,1] * norm_value_2\n",
        "  denorm_ecgs_3 = ecgs[0,:,2] * norm_value_3\n",
        "\n",
        "  fig, (ax1, ax2,ax3) = plt.subplots(1,3)\n",
        "\n",
        "  fig.set_figheight(4)\n",
        "  fig.set_figwidth(15)\n",
        "\n",
        "  ax1.plot(denorm_ecgs_1)\n",
        "  ax1.set(xlabel='Timestep', ylabel='Amplitude(mV)')\n",
        "  ax1.set_xlim((0,400))\n",
        "  ax1.set_title('Lead I')\n",
        "  ax1.grid()\n",
        "  ax2.plot(denorm_ecgs_2)\n",
        "  ax2.set(xlabel='Timestep', ylabel='Amplitude(mV)')\n",
        "  ax2.set_xlim((0,400))\n",
        "  ax2.set_title('Lead II')\n",
        "  ax2.grid()\n",
        "  ax3.plot(denorm_ecgs_3)\n",
        "  ax3.set(xlabel='Timestep', ylabel='Amplitude(mV)')\n",
        "  ax3.set_xlim((0,400))\n",
        "  ax3.set_title('Lead III')\n",
        "  ax3.grid()\n",
        "  \n",
        "  fig.tight_layout()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjCztqJ49eqD"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 2000\n",
        "critic = create_critic()\n",
        "generator = create_generator()\n",
        "GAN = create_GAN(critic, generator)\n",
        "\n",
        "\n",
        "noise_array = np.array(tf.random.normal([1000,400,1]))\n",
        "print('BREAK')\n",
        "print('noise_array.shape')\n",
        "print(noise_array.shape)\n",
        "print('BREAK')\n",
        "d_losses, d_accuracy, g_losses = [], [], []\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  \n",
        "  for _ in range(n_critic):\n",
        "    \n",
        "    idx = np.random.randint(0, high = 954, size= batch_size)\n",
        "    real_ecgs = data[idx]\n",
        "\n",
        "\n",
        "    idx = np.random.randint(0,high= 1000 ,size=batch_size)\n",
        "    noise = noise_array[idx,:,:]\n",
        "\n",
        "    fake_ecgs = generator.predict(noise,batch_size=batch_size)\n",
        "\n",
        "    X_real = real_ecgs\n",
        "    X_fake = fake_ecgs\n",
        "\n",
        "    y_real = -np.ones((batch_size,3)) \n",
        "    y_fake = np.ones((batch_size,3)) \n",
        "\n",
        "    critic.trainable = True\n",
        "    c_loss_real = critic.train_on_batch(X_real ,y_real)\n",
        "    c_loss_fake = critic.train_on_batch(X_fake ,y_fake)\n",
        "    critic.trainable = False\n",
        "\n",
        "    for l in critic.layers:\n",
        "        weights = l.get_weights()\n",
        "        weights = [np.clip(w, -clip_value, clip_value) for w in weights]\n",
        "        l.set_weights(weights)\n",
        "\n",
        "  y = - np.ones((batch_size,3))\n",
        "  g_loss = GAN.train_on_batch(noise, y)\n",
        "\n",
        "  \n",
        "  if epoch%50 == 0:\n",
        "    show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "    show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "    show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "    show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "    show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "    show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "    show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "   \n",
        "    \n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "  \n",
        "    # mmd_loss_1()\n",
        "    # mmd_loss_2()\n",
        "    # mmd_loss_3()\n",
        "\n",
        "plt.figure\n",
        "plt.plot(d_losses, 'r')\n",
        "plt.plot(g_losses, 'b')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(d_accuracy)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwIFSLGJLuot"
      },
      "source": [
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)\n",
        "show_results(generator,norm_value_1,norm_value_2,norm_value_3)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}